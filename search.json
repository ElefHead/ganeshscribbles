[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Ganesh, I am a machine learning engineer who likes ml systems and I collect my notes here."
  },
  {
    "objectID": "posts/flash-attention/2025-11-14-notes-on-flash-attention.html",
    "href": "posts/flash-attention/2025-11-14-notes-on-flash-attention.html",
    "title": "Notes on FlashAttention",
    "section": "",
    "text": "In this 2022 paper, the authors ask if efficiency in computing exact attention can enable transformers parse larger sequences and help overcome runtime and memory challenges for long sequences.\nThey note that\n\napproximate attention techiques, a common approach to overcoming the quadratic time and space complexity of self-attention, tend to ignore overheads from memory access (IO speeds in GPUs).\n\nas shown in Ivanov et. al, most operations on modern GPUs are bottlenecked by the memory bandwidth and fall short of using the available compute efficiently.\n\nThus, the authors argue for using operations that account for the reads and writes to different levels of fast and slow memory, i.e.¬†introduction of IO-awareness in algorithms, on modern accelerators (eg. SRAM vs High Bandwidth Memory in modern GPUs). With this, they focus on the attention computation to\n\nreduce the number of memory access by computing the exact softmax without multiple accesses to the whole input. This helps speed up the forward pass during training as well as inference.\nnot store the large intermediate attention matrix for the backward pass. This speeds up the gradient computation during training.\n\nThe authors introduce the FlashAttention kernel that uses tiling to compute softmax over blocks of the input using a softmax reduction that enables both (1) the incremental calculation of the final softmax matrix without multiple global memory access, as well as (2) storage of block statistics instead of the large intermediate attention matrix for the gradient computation. This also allows for operation fusion allowing the calculation of output attention matrix in one kernel.\nThey further prove that FlashAttention requires \\(O(N^2d^2M^{-1})\\) HBM accesses where \\(d\\) is the head dimension and \\(M\\) is the size of SRAM, as compared to \\(\\Omega(Nd + N^2)\\) of standard attention.\n\n\n\n\n\n\n\n\nTipSoftmax Formula\n\n\n\n\n\nGiven input sequences \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d}\\)\n\\[\\begin{align}\n\\mathbf{S} &= \\mathbf{Q}\\mathbf{K}^{T} &&\\in \\mathbb{R}^{N \\times N} \\\\\n\\mathbf{P} &= \\text{softmax}(\\mathbf{S}) &&\\in \\mathbb{R}^{N \\times N} \\\\\n\\mathbf{O} &= \\mathbf{P}\\mathbf{V} &&\\in \\mathbb{R}^{N \\times d}\n\\end{align}\\]\nwhere,\n\\(N\\) is the sequence length\n\\(d\\) is the head dimension\nOften, \\(N \\gg d\\) (e.g., for GPT2, \\(N = 1024\\) and \\(d = 64\\)).\n\n\n\nThe standard attention algorithm (given below) materializes the matrices \\(\\mathbf{S}\\) and \\(\\mathbf{P}\\) to the HBM which takes \\(O(N^2)\\) memory. As some or most of the operations are memory-bound, the large number of memory accesses translate to slow wall-clock time.\n\n\nRequire: Matrices \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V} \\in \\mathbb{R}^{N \\times d}\\) in HBM.\n\nLoad \\(\\mathbf{Q}, \\mathbf{K}\\) by blocks from HBM, compute \\(\\mathbf{S}\\) and write it to HBM.\nRead \\(\\mathbf{S}\\) from HBM, compute \\(P\\) and write it to HBM.\nLoad \\(\\mathbf{P}, \\mathbf{V}\\) by blocks from HBM, compute \\(\\mathbf{O}\\) and write to HBM.\nReturn \\(\\mathbf{O}\\).\n\n\n\n\n\nTo achieve the goal of computing attention with significantly reduced HBM access, the authors use tiling and re-computation.\n\nThe main idea is that we split the inputs \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) into blocks, load them from slow HBM to fast SRAM, then compute the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, we get the correct result at the end.\n\n\n\nThis is achieved by applying tiling to the online softmax reduction outlined by Milakov and Gimelshein.\n\n\n\n\n\n\nTipCalculating normalizer statistics within a block\n\n\n\n\n\nFor vector \\(x^{(k)} \\in \\mathbb{R}^B\\), \\(B\\) being the size of the block and \\(k\\) denoting that \\(x^{(k)}\\) \\(k_{\\text{th}}\\) block of some vector \\(x\\) of size \\(\\gg B\\)\n\\[\\begin{align}\nm(x^{(k)}) & := \\underset{i}{\\text{max }} x^{(k)}_i \\\\\nf(x^{(k)}) & := [e^{x^{(k)}_1 - m(x^{(k)})} ... e^{x^{(k)}_B - m(x^{(k)})}] \\\\\n\\ell(x^{(k)}) & := \\underset{i}{\\sum} f(x^{(k)})_i\n\\end{align}\\]\n\n\n\nThe block-wise calculations can be used to calculate the softmax of the full matrix using the normalization statistics \\((m(x), \\ell(x))\\).\n\n\n\n\n\n\nTipOnline softmax using normalizers\n\n\n\n\n\nFor vectors \\(x^{(1)}, x^{(2)} \\in \\mathbb{R}^B\\), the softmax of the concatenated \\(x = [x^{(1)}  x^{(2)}] \\in \\mathbb{R}^{2B}\\) as:\n\\[\\begin{align}\nm(x) &= m([x^{(1)} x^{(2)}]) &&= \\text{max}(m(x^{(1)}), m(x^{(2)})) \\\\\n\\ell(x) &= \\ell([x^{(1)} x^{(2)}]) &&= e^{m(x^{(1)}) - m(x)}\\ell(x^{(1)}) e^{m(x^{(2)}) - m(x)}\\ell(x^{(2)})\n\\end{align}\\]\nUsing \\(m(x)\\) and \\(\\ell(x)\\), \\(f(x)\\) and \\(\\text{softmax}(x)\\) can be calculated as:\n\\[\\begin{align}\nf(x) &= [e^{m(x^{(1)}) - m(x)}f(x^{(1)}) e^{m(x^{(2)}) - m(x)}f(x^{(2)})] \\\\\n\\text{softmax}(x) &= \\frac{f(x)}{\\ell(x)}\n\\end{align}\\]\n\n\n\nThe following is a small hand-worked example: \n\n\n\n\n\n\nImportantKernel / Operation Fusion\n\n\n\nTiling enables the implementation of all computation steps in one kernel without multiple HBM access for reads and writes, i.e., it enables us to load input from HBM, perform computation (matrix multiply, softmax, optionally masking and dropout, matrix multiply), then write result back to HBM in one kernel.\n\n\n\n\n\nWhile the tiling trick enables speed-ups for inference, i.e.¬†during the forward pass through the transformer, the backward pass typically relies on the intermediate matrices \\(S,P \\in \\mathbb{R}^{N \\times N}\\) for the gradient computation with respect to \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\).\nTo avoid storing the \\(O(N^2)\\) intermediate values for the backward pass, the authors instead store the softmax normalization statistics \\((m, \\ell)\\) and re-compute \\(\\mathbf{S}, \\mathbf{P}\\) in the SRAM during the backward pass.\n\n\n\n\n\n\nImportant\n\n\n\nThe authors are increasing FLOPs in order to re-compute \\(\\mathbf{S}\\) and \\(\\mathbf{P}\\) fully in the SRAM and argue that this trade-off leads to more efficient use of the available hardware leading to an increased overall performance.\n\n\n\n\n\nThe authors prove that this algorithm returns \\(\\mathbf{O} = \\text{softmax}(\\mathbf{Q}\\mathbf{K}^T\\mathbf{V})\\) with \\(ùëÇ(ùëÅ^2d)\\) FLOPs and requires \\(ùëÇ(ùëÅ)\\) additional memory beyond inputs and output to store \\(m\\) and \\(\\ell\\).\nRequire: Matrices \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d}\\) in HBM, on-chip SRAM of size \\(M\\).\n\nSet block sizes \\(B_c = \\lceil \\frac{M}{4d} \\rceil\\), \\(B_r = \\min(\\lceil \\frac{M}{4d} \\rceil, d)\\).\nInitialize \\(\\mathbf{O} = (0)_{N \\times d} \\in \\mathbb{R}^{N \\times d}\\), \\(\\ell = (0)_{N} \\in \\mathbb{R}^{N}\\), \\({m} = (-\\infty)_{N} \\in \\mathbb{R}^{N}\\) in HBM.\nDivide \\(\\mathbf{Q}\\) into \\(T_r = \\lceil \\frac{N}{B_r} \\rceil\\) blocks \\(\\mathbf{Q}_1, \\dots, \\mathbf{Q}_{T_r}\\), of size \\(B_r \\times d\\) each, and divide \\(\\mathbf{K}, \\mathbf{V}\\) into \\(T_c = \\lceil \\frac{N}{B_c} \\rceil\\) blocks \\(\\mathbf{K}_1, \\dots, \\mathbf{K}_{T_c}\\) and \\(\\mathbf{V}_1, \\dots, \\mathbf{V}_{T_c}\\), of size \\(B_c \\times d\\) each.\nDivide \\(\\mathbf{O}\\) into \\(T_r\\) blocks \\(\\mathbf{O}_1, \\dots, \\mathbf{O}_{T_r}\\), of size \\(B_r \\times d\\) each, divide \\(\\ell\\) into \\(T_r\\) blocks \\(\\ell_1, \\dots, \\ell_{T_r}\\), of size \\(B_r\\) each, divide \\(m\\) into \\(T_r\\) blocks \\(m_1, \\dots, m_{T_r}\\), of size \\(B_r\\) each.\nfor \\(1 \\le j \\le T_c\\) do\n\\(\\quad\\) Load \\(\\mathbf{K}_j, \\mathbf{V}_j\\) from HBM to on-chip SRAM.\n\\(\\quad\\) for \\(1 \\le i \\le T_r\\) do\n\\(\\qquad\\) Load \\(\\mathbf{Q}_i, \\mathbf{O}_i, \\ell_i, m_i\\) from HBM to on-chip SRAM.\n\\(\\qquad\\) On chip, compute \\(\\mathbf{S}_{ij} = \\mathbf{Q}_i \\mathbf{K}_j^T \\in \\mathbb{R}^{B_r \\times B_c}\\).\n\\(\\qquad\\) On chip, compute \\(\\tilde{m}_{ij} = \\text{rowmax}(\\mathbf{S}_{ij}) \\in \\mathbb{R}^{B_r}\\), \\(\\mathbf{\\tilde{P}}_{ij} = \\exp(\\mathbf{S}_{ij} - \\tilde{m}_{ij}) \\in \\mathbb{R}^{B_r \\times B_c}\\) (pointwise), \\(\\mathbf{\\tilde{\\ell}}_{ij} = \\text{rowsum}(\\mathbf{\\tilde{P}}_{ij}) \\in \\mathbb{R}^{B_r}\\).\n\\(\\qquad\\) On chip, compute \\(m_i^{\\text{new}} = \\max(m_i, \\tilde{m}_{ij}) \\in \\mathbb{R}^{B_r}\\), \\(\\ell_i^{\\text{new}} = e^{m_i - m_i^{\\text{new}}} \\ell_i + e^{\\tilde{m}_{ij} - m_i^{\\text{new}}} \\mathbf{\\tilde{\\ell}}_{ij} \\in \\mathbb{R}^{B_r}\\).\n\\(\\qquad\\) Write \\(\\mathbf{O}_i \\leftarrow \\text{diag}(\\ell_i^{\\text{new}})^{-1} (\\text{diag}(\\ell_i) e^{m_i - m_i^{\\text{new}}} \\mathbf{O}_i + e^{\\tilde{m}_{ij} - m_i^{\\text{new}}} \\mathbf{\\tilde{P}}_{ij} \\mathbf{V}_j)\\) to HBM.\n\\(\\qquad\\) Write \\(\\ell_i \\leftarrow \\ell_i^{\\text{new}}\\), \\(m_i \\leftarrow m_i^{\\text{new}}\\) to HBM.\n\\(\\quad\\) end for\nend for\nReturn \\(\\mathbf{O}\\).\n\n\n\n\n\n\n\nTipCalculating block and tile sizes\n\n\n\n\n\nAn A100 has a configurable 164KB SRAM per SM.\nAssuming the availability of 128KB and using fp16/bf16 (2 bytes to represent a number) for all our computations,\nwe can calculate \\(M\\) first: \\[\\begin{align}\nM &= \\frac{\\text{Total Available Bytes}}{\\text{ Bytes to represent a number}}&&= \\frac{128 \\times 1024}{2} &&= 65536\n\\end{align}\\]\nWith \\(M\\), we can calculate \\(B_c\\) and \\(B_r\\): \\[\\begin{align}\nB_c &= \\lceil \\frac{M}{4 \\times d} \\rceil &&= \\lceil \\frac{65536}{4 \\times 64} \\rceil &&= 256 \\\\\nB_r &= \\min(\\lceil \\frac{M}{4 \\times d} \\rceil, d) &&= \\min(\\lceil \\frac{65536}{4 \\times 64} \\rceil, 64) &&= 64\n\\end{align}\\]\nAssuming \\(N = 1024\\) (used in the paper), we can calculate tile sizes as:\n\\[\\begin{align}\nT_r &= \\lceil \\frac{N}{B_r} \\rceil &&= 1024 / 64&&= 16 \\\\\nT_c &= \\lceil \\frac{N}{B_c} \\rceil &&= 1024 / 256&&= 4\n\\end{align}\\]\n\n\n\n\n\n\n\nIn the paper, the authors show the effect of the forward runtime of FlashAttention using the settings on an A100 GPU\n\nsequence length, \\(N = 1024\\)\nhead dimensions, \\(d = 64\\)\nnum_heads \\(= 16\\)\nbatch_size \\(= 64\\)\n\nFrom the graph, it is visible that:\n\nIncreasing the block size decreases the HBM accesses.\nFewer HBM accesses result in faster runtime, up to a point - this is most likely because at higher block sizes the bottlenecks shift from memory bandwidth (to arithmetic operations in the ideal case, or, in the worst case, resource constraints).\n\nIt is useful to consider the implications using commonly used GPUs.\n\nCommonly used NVIDIA GPUs with their SRAM, VRAM, and IO Bandwidth numbers from their white papers.\n\n\nGPU\nArch\nSMs\nSRAM[^]\nVRAM\nMemory Bandwidth\n\n\n\n\nT4\nTuring TU104\n46\n64KB\n16 GB\n616 GB/sec\n\n\nV100\nVolta GV100\n80\n96KB\n16 GB\n760 GB/sec\n\n\nA10g\nAmpere GA102\n84\n96KB\n24 GB\n760 GB/sec\n\n\nA100\nAmpere A100\n108\n164KB\n40 / 80 GB\n1555 GB/sec\n\n\nH100\nHopper H100\n114\n228KB\n80 GB\n2039 GB/sec\n\n\n\nJust by looking at these numbers, there a few things that we can expect about performance gains:\n\nDuring Inference: On a GPU like T4, even small models like llama3.2-3B that can fit within its VRAM will see only marginal gains at full sequence length (if any) because the block size will have to be very small due to the small SRAM. Any real improvements would need exploit additional characteristics of inference settings.\nDuring Training: On T4, there is likely no performance gain to be expected when dealing with realistic sequence lengths. The A10 might be only a slightly better budget candidate to T4 even at twice the cost per GPU, but any real training would need something like an A100 or H100.\nOn the other hand, very small models / small models in small sequence settings, if run on GPUs like A/H100 using all of the available resource, might not be memory bound in which case FlashAttention might add additional computation steps."
  },
  {
    "objectID": "posts/flash-attention/2025-11-14-notes-on-flash-attention.html#standard-attention",
    "href": "posts/flash-attention/2025-11-14-notes-on-flash-attention.html#standard-attention",
    "title": "Notes on FlashAttention",
    "section": "",
    "text": "TipSoftmax Formula\n\n\n\n\n\nGiven input sequences \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d}\\)\n\\[\\begin{align}\n\\mathbf{S} &= \\mathbf{Q}\\mathbf{K}^{T} &&\\in \\mathbb{R}^{N \\times N} \\\\\n\\mathbf{P} &= \\text{softmax}(\\mathbf{S}) &&\\in \\mathbb{R}^{N \\times N} \\\\\n\\mathbf{O} &= \\mathbf{P}\\mathbf{V} &&\\in \\mathbb{R}^{N \\times d}\n\\end{align}\\]\nwhere,\n\\(N\\) is the sequence length\n\\(d\\) is the head dimension\nOften, \\(N \\gg d\\) (e.g., for GPT2, \\(N = 1024\\) and \\(d = 64\\)).\n\n\n\nThe standard attention algorithm (given below) materializes the matrices \\(\\mathbf{S}\\) and \\(\\mathbf{P}\\) to the HBM which takes \\(O(N^2)\\) memory. As some or most of the operations are memory-bound, the large number of memory accesses translate to slow wall-clock time.\n\n\nRequire: Matrices \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V} \\in \\mathbb{R}^{N \\times d}\\) in HBM.\n\nLoad \\(\\mathbf{Q}, \\mathbf{K}\\) by blocks from HBM, compute \\(\\mathbf{S}\\) and write it to HBM.\nRead \\(\\mathbf{S}\\) from HBM, compute \\(P\\) and write it to HBM.\nLoad \\(\\mathbf{P}, \\mathbf{V}\\) by blocks from HBM, compute \\(\\mathbf{O}\\) and write to HBM.\nReturn \\(\\mathbf{O}\\)."
  },
  {
    "objectID": "posts/flash-attention/2025-11-14-notes-on-flash-attention.html#flashattention",
    "href": "posts/flash-attention/2025-11-14-notes-on-flash-attention.html#flashattention",
    "title": "Notes on FlashAttention",
    "section": "",
    "text": "To achieve the goal of computing attention with significantly reduced HBM access, the authors use tiling and re-computation.\n\nThe main idea is that we split the inputs \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) into blocks, load them from slow HBM to fast SRAM, then compute the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, we get the correct result at the end.\n\n\n\nThis is achieved by applying tiling to the online softmax reduction outlined by Milakov and Gimelshein.\n\n\n\n\n\n\nTipCalculating normalizer statistics within a block\n\n\n\n\n\nFor vector \\(x^{(k)} \\in \\mathbb{R}^B\\), \\(B\\) being the size of the block and \\(k\\) denoting that \\(x^{(k)}\\) \\(k_{\\text{th}}\\) block of some vector \\(x\\) of size \\(\\gg B\\)\n\\[\\begin{align}\nm(x^{(k)}) & := \\underset{i}{\\text{max }} x^{(k)}_i \\\\\nf(x^{(k)}) & := [e^{x^{(k)}_1 - m(x^{(k)})} ... e^{x^{(k)}_B - m(x^{(k)})}] \\\\\n\\ell(x^{(k)}) & := \\underset{i}{\\sum} f(x^{(k)})_i\n\\end{align}\\]\n\n\n\nThe block-wise calculations can be used to calculate the softmax of the full matrix using the normalization statistics \\((m(x), \\ell(x))\\).\n\n\n\n\n\n\nTipOnline softmax using normalizers\n\n\n\n\n\nFor vectors \\(x^{(1)}, x^{(2)} \\in \\mathbb{R}^B\\), the softmax of the concatenated \\(x = [x^{(1)}  x^{(2)}] \\in \\mathbb{R}^{2B}\\) as:\n\\[\\begin{align}\nm(x) &= m([x^{(1)} x^{(2)}]) &&= \\text{max}(m(x^{(1)}), m(x^{(2)})) \\\\\n\\ell(x) &= \\ell([x^{(1)} x^{(2)}]) &&= e^{m(x^{(1)}) - m(x)}\\ell(x^{(1)}) e^{m(x^{(2)}) - m(x)}\\ell(x^{(2)})\n\\end{align}\\]\nUsing \\(m(x)\\) and \\(\\ell(x)\\), \\(f(x)\\) and \\(\\text{softmax}(x)\\) can be calculated as:\n\\[\\begin{align}\nf(x) &= [e^{m(x^{(1)}) - m(x)}f(x^{(1)}) e^{m(x^{(2)}) - m(x)}f(x^{(2)})] \\\\\n\\text{softmax}(x) &= \\frac{f(x)}{\\ell(x)}\n\\end{align}\\]\n\n\n\nThe following is a small hand-worked example: \n\n\n\n\n\n\nImportantKernel / Operation Fusion\n\n\n\nTiling enables the implementation of all computation steps in one kernel without multiple HBM access for reads and writes, i.e., it enables us to load input from HBM, perform computation (matrix multiply, softmax, optionally masking and dropout, matrix multiply), then write result back to HBM in one kernel.\n\n\n\n\n\nWhile the tiling trick enables speed-ups for inference, i.e.¬†during the forward pass through the transformer, the backward pass typically relies on the intermediate matrices \\(S,P \\in \\mathbb{R}^{N \\times N}\\) for the gradient computation with respect to \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\).\nTo avoid storing the \\(O(N^2)\\) intermediate values for the backward pass, the authors instead store the softmax normalization statistics \\((m, \\ell)\\) and re-compute \\(\\mathbf{S}, \\mathbf{P}\\) in the SRAM during the backward pass.\n\n\n\n\n\n\nImportant\n\n\n\nThe authors are increasing FLOPs in order to re-compute \\(\\mathbf{S}\\) and \\(\\mathbf{P}\\) fully in the SRAM and argue that this trade-off leads to more efficient use of the available hardware leading to an increased overall performance.\n\n\n\n\n\nThe authors prove that this algorithm returns \\(\\mathbf{O} = \\text{softmax}(\\mathbf{Q}\\mathbf{K}^T\\mathbf{V})\\) with \\(ùëÇ(ùëÅ^2d)\\) FLOPs and requires \\(ùëÇ(ùëÅ)\\) additional memory beyond inputs and output to store \\(m\\) and \\(\\ell\\).\nRequire: Matrices \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d}\\) in HBM, on-chip SRAM of size \\(M\\).\n\nSet block sizes \\(B_c = \\lceil \\frac{M}{4d} \\rceil\\), \\(B_r = \\min(\\lceil \\frac{M}{4d} \\rceil, d)\\).\nInitialize \\(\\mathbf{O} = (0)_{N \\times d} \\in \\mathbb{R}^{N \\times d}\\), \\(\\ell = (0)_{N} \\in \\mathbb{R}^{N}\\), \\({m} = (-\\infty)_{N} \\in \\mathbb{R}^{N}\\) in HBM.\nDivide \\(\\mathbf{Q}\\) into \\(T_r = \\lceil \\frac{N}{B_r} \\rceil\\) blocks \\(\\mathbf{Q}_1, \\dots, \\mathbf{Q}_{T_r}\\), of size \\(B_r \\times d\\) each, and divide \\(\\mathbf{K}, \\mathbf{V}\\) into \\(T_c = \\lceil \\frac{N}{B_c} \\rceil\\) blocks \\(\\mathbf{K}_1, \\dots, \\mathbf{K}_{T_c}\\) and \\(\\mathbf{V}_1, \\dots, \\mathbf{V}_{T_c}\\), of size \\(B_c \\times d\\) each.\nDivide \\(\\mathbf{O}\\) into \\(T_r\\) blocks \\(\\mathbf{O}_1, \\dots, \\mathbf{O}_{T_r}\\), of size \\(B_r \\times d\\) each, divide \\(\\ell\\) into \\(T_r\\) blocks \\(\\ell_1, \\dots, \\ell_{T_r}\\), of size \\(B_r\\) each, divide \\(m\\) into \\(T_r\\) blocks \\(m_1, \\dots, m_{T_r}\\), of size \\(B_r\\) each.\nfor \\(1 \\le j \\le T_c\\) do\n\\(\\quad\\) Load \\(\\mathbf{K}_j, \\mathbf{V}_j\\) from HBM to on-chip SRAM.\n\\(\\quad\\) for \\(1 \\le i \\le T_r\\) do\n\\(\\qquad\\) Load \\(\\mathbf{Q}_i, \\mathbf{O}_i, \\ell_i, m_i\\) from HBM to on-chip SRAM.\n\\(\\qquad\\) On chip, compute \\(\\mathbf{S}_{ij} = \\mathbf{Q}_i \\mathbf{K}_j^T \\in \\mathbb{R}^{B_r \\times B_c}\\).\n\\(\\qquad\\) On chip, compute \\(\\tilde{m}_{ij} = \\text{rowmax}(\\mathbf{S}_{ij}) \\in \\mathbb{R}^{B_r}\\), \\(\\mathbf{\\tilde{P}}_{ij} = \\exp(\\mathbf{S}_{ij} - \\tilde{m}_{ij}) \\in \\mathbb{R}^{B_r \\times B_c}\\) (pointwise), \\(\\mathbf{\\tilde{\\ell}}_{ij} = \\text{rowsum}(\\mathbf{\\tilde{P}}_{ij}) \\in \\mathbb{R}^{B_r}\\).\n\\(\\qquad\\) On chip, compute \\(m_i^{\\text{new}} = \\max(m_i, \\tilde{m}_{ij}) \\in \\mathbb{R}^{B_r}\\), \\(\\ell_i^{\\text{new}} = e^{m_i - m_i^{\\text{new}}} \\ell_i + e^{\\tilde{m}_{ij} - m_i^{\\text{new}}} \\mathbf{\\tilde{\\ell}}_{ij} \\in \\mathbb{R}^{B_r}\\).\n\\(\\qquad\\) Write \\(\\mathbf{O}_i \\leftarrow \\text{diag}(\\ell_i^{\\text{new}})^{-1} (\\text{diag}(\\ell_i) e^{m_i - m_i^{\\text{new}}} \\mathbf{O}_i + e^{\\tilde{m}_{ij} - m_i^{\\text{new}}} \\mathbf{\\tilde{P}}_{ij} \\mathbf{V}_j)\\) to HBM.\n\\(\\qquad\\) Write \\(\\ell_i \\leftarrow \\ell_i^{\\text{new}}\\), \\(m_i \\leftarrow m_i^{\\text{new}}\\) to HBM.\n\\(\\quad\\) end for\nend for\nReturn \\(\\mathbf{O}\\).\n\n\n\n\n\n\n\nTipCalculating block and tile sizes\n\n\n\n\n\nAn A100 has a configurable 164KB SRAM per SM.\nAssuming the availability of 128KB and using fp16/bf16 (2 bytes to represent a number) for all our computations,\nwe can calculate \\(M\\) first: \\[\\begin{align}\nM &= \\frac{\\text{Total Available Bytes}}{\\text{ Bytes to represent a number}}&&= \\frac{128 \\times 1024}{2} &&= 65536\n\\end{align}\\]\nWith \\(M\\), we can calculate \\(B_c\\) and \\(B_r\\): \\[\\begin{align}\nB_c &= \\lceil \\frac{M}{4 \\times d} \\rceil &&= \\lceil \\frac{65536}{4 \\times 64} \\rceil &&= 256 \\\\\nB_r &= \\min(\\lceil \\frac{M}{4 \\times d} \\rceil, d) &&= \\min(\\lceil \\frac{65536}{4 \\times 64} \\rceil, 64) &&= 64\n\\end{align}\\]\nAssuming \\(N = 1024\\) (used in the paper), we can calculate tile sizes as:\n\\[\\begin{align}\nT_r &= \\lceil \\frac{N}{B_r} \\rceil &&= 1024 / 64&&= 16 \\\\\nT_c &= \\lceil \\frac{N}{B_c} \\rceil &&= 1024 / 256&&= 4\n\\end{align}\\]\n\n\n\n\n\n\n\nIn the paper, the authors show the effect of the forward runtime of FlashAttention using the settings on an A100 GPU\n\nsequence length, \\(N = 1024\\)\nhead dimensions, \\(d = 64\\)\nnum_heads \\(= 16\\)\nbatch_size \\(= 64\\)\n\nFrom the graph, it is visible that:\n\nIncreasing the block size decreases the HBM accesses.\nFewer HBM accesses result in faster runtime, up to a point - this is most likely because at higher block sizes the bottlenecks shift from memory bandwidth (to arithmetic operations in the ideal case, or, in the worst case, resource constraints).\n\nIt is useful to consider the implications using commonly used GPUs.\n\nCommonly used NVIDIA GPUs with their SRAM, VRAM, and IO Bandwidth numbers from their white papers.\n\n\nGPU\nArch\nSMs\nSRAM[^]\nVRAM\nMemory Bandwidth\n\n\n\n\nT4\nTuring TU104\n46\n64KB\n16 GB\n616 GB/sec\n\n\nV100\nVolta GV100\n80\n96KB\n16 GB\n760 GB/sec\n\n\nA10g\nAmpere GA102\n84\n96KB\n24 GB\n760 GB/sec\n\n\nA100\nAmpere A100\n108\n164KB\n40 / 80 GB\n1555 GB/sec\n\n\nH100\nHopper H100\n114\n228KB\n80 GB\n2039 GB/sec\n\n\n\nJust by looking at these numbers, there a few things that we can expect about performance gains:\n\nDuring Inference: On a GPU like T4, even small models like llama3.2-3B that can fit within its VRAM will see only marginal gains at full sequence length (if any) because the block size will have to be very small due to the small SRAM. Any real improvements would need exploit additional characteristics of inference settings.\nDuring Training: On T4, there is likely no performance gain to be expected when dealing with realistic sequence lengths. The A10 might be only a slightly better budget candidate to T4 even at twice the cost per GPU, but any real training would need something like an A100 or H100.\nOn the other hand, very small models / small models in small sequence settings, if run on GPUs like A/H100 using all of the available resource, might not be memory bound in which case FlashAttention might add additional computation steps."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ganeshscribbles",
    "section": "",
    "text": "Notes on FlashAttention\n\n\n\ninference\n\nattention\n\nperformance\n\n\n\nUnderstanding the FlashAttention papers and the various implementations.\n\n\n\n\n\nNov 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMlflow : Design Decisions and Tricky Parts\n\n\n\nexperimentation\n\nmlflow\n\npython\n\n\n\nChoices to keep in mind while adding mlflow logging to a codebase\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/mlflow-logging/2022-09-12-mlflow-logging-design-decisions-tricky-parts.html",
    "href": "posts/mlflow-logging/2022-09-12-mlflow-logging-design-decisions-tricky-parts.html",
    "title": "Mlflow : Design Decisions and Tricky Parts",
    "section": "",
    "text": "I have been using Mlflow for a while now to track experimentation and model artifact lineage. While setting up codebases to use mlflow, I had to make some design decisions that influenced the successful logging of all metrics, parameters, artifacts, and the addition of the model artifacts to the model registry.\nBroadly, these were:\n\nThe intended pattern to run projects\n\nLogging artifacts the right way\n\nI wanted to draw out a few of these by adding mlflow logging to a small script. Some of these choices become gotchas while working with large codebases that have long data loading and training periods. Depending on the task and the compute being used, these gotchas could be expensive.\n\nPre-requisites\nLet‚Äôs start with a function that doesn‚Äôt do any machine learning, it simply takes in a parameter (an integer index) and iterates upto that number while printing the numbers to a file.\ndef iter_and_log(index: int, log_file: Path):\n    with open(log_file, \"w\") as lf:\n        for i in range(index):\n            lf.write(f\"{i}\\n\")\nA commonly followed pattern in machine learning / deep learning training code is:\n\nAccept parameters from cli using packages like argparse or click.\nCreate some artifacts (our log file here is an artifact that we can use)\nAfter performing training (also during), save the model.\n\nFor the purpose of our example, let‚Äôs mimic this pattern by doing the following:\n## iterate.py \n\nimport click\nfrom pathlib import Path\nimport pickle\n\n@click.command()\n@click.option(\n    \"--index\",\n    \"-i\",\n    type=click.INT,\n    help=\"Upper limit for iteration\",\n)\n@click.option(\n    \"--log-dir\",\n    \"-L\",\n    type=click.Path(),\n    default=Path(\"log_artifacts\"),\n    help=\"Log artifact file path\"\n)\n@click.option(\n    \"--model-dir\",\n    \"-M\",\n    type=click.Path(),\n    default=Path(\"model_dir\"),\n    help=\"'Model' directory\"\n)\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    # ensure log_dir and model_dir are Path objects\n    log_dir = Path(log_dir)\n    model_dir = Path(model_dir)\n\n    ## Create dirs if they don't exist\n    if not log_dir.exists(): log_dir.mkdir()\n    if not model_dir.exists(): model_dir.mkdir()\n\n    # set log_file path\n    artifact_file = log_dir / \"log_file.txt\"\n\n    # perform iteration and logging\n    iter_and_log(index, artifact_file)\n\n    # serialize and save the function that does serialization and logging\n    # this is our proxy for a model\n    with open(model_dir / \"model_pickle\", \"wb\") as model_file:\n        pickle.dump(iter_and_log, model_file)\n\ndef iter_and_log(index: int, log_file: Path) -&gt; None:\n    \"\"\"Function that does the iteration and logging\n    Iterates to `index` and logs the values to `log_file`\n    \"\"\"\n    with open(log_file, \"w\") as lf:\n        for i in range(index):\n            lf.write(f\"{i}\\n\")\n    \n\nif __name__ == \"__main__\":\n    iterate()\nThis can be run for 10 iterations as follows\npython iterate.py -i 10\nand it will create the artifact directories, iterate and log, and then serialize and dump the function to file.\nNow, with the pre-requisits done, let‚Äôs talk about adding mlflow logging and running the project.\n\n\nThe Intended Pattern to Run Projects\nA project that has mlflow logging can be run using two patterns\n\nUsing the python command\n\nIn this pattern, the project is run as it would be normally i.e.¬†its python command or bash script. In our example that would be python iterate.py -i 10.\n\nWhen logging is done this way, mlflow runs in Fluent API mode.\n\nMlflow Projects\n\nIn this pattern, an MLproject file gets defined at the project‚Äôs root directory.\n\nTo run the project, the mlflow run command is used to run an entry point and the command to be used is defined within the entry point.\n\n\nAdding logging to the codebase such that both of these patterns can be supported is often tricky! Let‚Äôs add logging to our script and see why. The first step is adding the MLproject file:\n## MLproject\n\nname: sample-mlflow-iterate\n\nentry_points:\n  main:\n    parameters:\n      index: int\n      log_dir:\n        default: log_artifacts\n      model_dir:\n        default: model_dir\n\n    command: python iterate.py -i {index} --log-dir {log_dir} --model-dir {model_dir}\nOnce, the file has been added to the project root, we can use mlflow cli to run the project as\nmlflow run . -P index=10 --env-manager=local\nwhere -P is how a parameter is specified while running and --env-manager=local is to tell mlflow to use the local environment.\nCircling back, our project that will use mlflow logging, is able to run in two ways:\n1. Using the python command python iterate.py -i 10, in which case mlflow will run as Fluent API.\n2. Using the Mlproject file and mlflow cli as mlflow run . -P index=10 --env-manager=local.\nNow, let‚Äôs add logging and see the gotchas depending on how we run the projects.\n\nStarting a run and logging metrics\nTo start logging to a run, an mlflow run has to be started in the code. We can do that using mlflow.start_run() either as a context using the with keyword or by itself. The documentation for this function does it using the with keyword - which also takes care of ending the run. But in many codebases (eg. huggingface transformers) encapsulating the entire logging process inside of one context is not possible, and therefore mlflow.start_run() is used by itself quite often. When we do that, we also need to take care that we end the run using mlflow.end_run().\nOur script is small, so let‚Äôs use the with keyword:\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    # ensure log_dir and model_dir are Path objects\n    log_dir = Path(log_dir)\n    model_dir = Path(model_dir)\n\n    ## Create dirs if they don't exist\n    if not log_dir.exists(): log_dir.mkdir()\n    if not model_dir.exists(): model_dir.mkdir()\n\n    # set log_file path\n    artifact_file = log_dir / \"log_file.txt\"\n\n    # start mlflow run\n    with mlflow.start_run():\n\n        # perform iteration and logging\n        iter_and_log(index, artifact_file)\n\n        # serialize and save the function that does serialization and logging\n        # this is our proxy for a model\n        with open(model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\nand to log some metrics, we can use mlflow.log_metric() function within the iter_and_log function as follows:\ndef iter_and_log(index: int, log_file: Path) -&gt; None:\n    \"\"\"Function that does the iteration and logging\n    Iterates to `index` and logs the values to `log_file`\n    \"\"\"\n    with open(log_file, \"w\") as lf:\n        for i in range(index):\n            # log to mlflow\n            mlflow.log_metric(\n                key=\"current_iter\",\n                value=i,\n                step=i # Don't forget to add step, absence makes all logs to step 0\n            )\n\n            lf.write(f\"{i}\\n\")\nNow when we run this project using mlflow run, we can see that mlflow logs the input parameters (specified in MLproject) and the metrics as we intended. \nBut when we run the same using the python command, we see that the parameters are not logged. \nFixing that should be as easy as using mlflow.log_param() in the code, right? In this script - yes. Let‚Äôs add that before we talk about where that could be insufficient. We can modify the iterate function to add parameter logging.\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    # ensure log_dir and model_dir are Path objects\n    log_dir = Path(log_dir)\n    model_dir = Path(model_dir)\n\n    ## Create dirs if they don't exist\n    if not log_dir.exists(): log_dir.mkdir()\n    if not model_dir.exists(): model_dir.mkdir()\n\n    # set log_file path\n    artifact_file = log_dir / \"log_file.txt\"\n\n    # start mlflow run\n    with mlflow.start_run():\n        # Log parameters\n        mlflow.log_param(key=\"index\", value=index)\n        mlflow.log_param(key=\"log_dir\", value=str(log_dir))\n        mlflow.log_param(key=\"model_dir\", value=str(model_dir))\n\n        # perform iteration and logging\n        iter_and_log(index, artifact_file)\n\n        # serialize and save the function that does serialization and logging\n        # this is our proxy for a model\n        with open(model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\nand with this we can run the script using the python command and even the parameters will be logged. \n\n\nReasonable defaults and logging parameters\nIn Mlflow, the intended use for logging parameters seems to be only for the purpose of capturing initial parameters and hyperparameters. Unlike metric logging, the parameter logging functions don‚Äôt have a step parameter to capture parameter values as they change. There is also no way to update a subset of parameters which is possible in some other logging libraries such as Weights and Biases. In mlflow, (hyper)parameters that change over time should be logged as metrics (if they are numeric) or to a nested run as recommended by the package.\nBut a common practice in many codebases is to set reasonable defaults and rely on those to be used when an empty string or a null value is passed as an initial parameter. Codebases such as fairseq have dozens of parameters and it‚Äôs not realistic to assume that a user would set them all when running the code. Likewise, it‚Äôs not realistic to assume that a user would write them all in the MLproject file. A more realistic assumption is that the user would specify some of the parameters and rely on the default values for the remaining.\nLet‚Äôs mimic this pattern in our script and see what happens.\nFirst we modify our iterate function as\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    ## set reasonable defaults\n    if log_dir is None or str(log_dir).strip() == \"\": log_dir = Path(\"log_artifacts\")\n    if model_dir is None or str(model_dir).strip() == \"\": model_dir = Path(\"model_dir\")\n\n    # ensure log_dir and model_dir are Path objects\n    log_dir = Path(log_dir)\n    model_dir = Path(model_dir)\n\n    ## Create dirs if they don't exist\n    if not log_dir.exists(): log_dir.mkdir()\n    if not model_dir.exists(): model_dir.mkdir()\n\n    # set log_file path\n    artifact_file = log_dir / \"log_file.txt\"\n\n    # start mlflow run\n    with mlflow.start_run():\n\n        # perform iteration and logging\n        iter_and_log(index, artifact_file)\n\n        # serialize and save the function that does serialization and logging\n        # this is our proxy for a model\n        with open(model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\nWe know that this already works using the python command because we have been using the defaults specified using click. But in the MLproject file, we have specified all the parameters with their defaults. Let‚Äôs modify it to get the code to rely on default values.\n## MLproject\n\nname: sample-mlflow-iterate\n\nentry_points:\n  main:\n    parameters:\n      index: int\n      model_dir:\n        default: null\n\n    command: python iterate.py -i {index} --model-dir {model_dir}\nI modified the MLproject file to remove log_dir altogether and set model_dir to null. When we now run this code using mlflow run . -P index=10 --env-manager=local, turns out it is not allowed:\n2022/09/11 18:33:40 ERROR mlflow.cli: === No value given for missing parameters: 'model_dir' ===\nWe cannot set parameters to null. This means, if a codebase requires some parameter be set to None to get the code to use a default values, leave it out of the MLproject file. This might actually be impossible in many codebases and should be taken under consideration when deciding to support running the project using the MLproject file.\nLuckily, our script also replaces an empty string with defaults. Therefore, we should be able to modify the MLproject file and set model_dir to ''.\n## MLproject\n\nname: sample-mlflow-iterate\n\nentry_points:\n  main:\n    parameters:\n      index: int\n      model_dir:\n        default: \"\"\n\n    command: python iterate.py -i {index} --model-dir {model_dir}\nWhen we now run this, we still run into an error:\nmlflow.exceptions.MlflowException: Changing param values is not allowed. \\ \nParam with key='model_dir' was already logged with \\\nvalue='''' for run ID='7e8922428cf5464582b6f44fb3a183ab'. \\ \nAttempted logging new value 'model_dir'.\nThis is because model_dir was already logged from MLProject file and so when we log the value we finally use, it raises the error to indicate that updating a parameter is not allowed.\nIn my opinion, there is no ideal way to fix this. I dislike the idea of logging this value to a nested run, since there is no need. The parameter did not evolve over time to trigger a search in a new hyperparameter space, this is just getting the code to use a default value. But here are a few possible ways to address this:\n1. Ignore logging the parameters that have already been logged\nHere is some code that could do that assuming the parameters have been specified as a dictionary in the code:\nfrom mlflow.tracking import MlflowClient\n\nrun_id = mlflow.active_run().info.run_id # get run_id of active run\nrun = self.client.get_run(run_id=self.run_id) # get current run data\nlogged_params = run.data.params # get already logged params as a dict\nfor k, v in params_dict.items():\n    if k not in logged_params:\n        mlflow.log_param(key=k, value=str(v))\n2. Use different names for parameters and the command line arguments\nThis allows both, command line arguments that are initial parameters, as well as the values that actually get used to be logged. Example: Fairseq uses the command line arguments to set the configs that are actually used in code. It also provides the cli args as part of the cfg variable, and so when logging parameters using mlflow for fairseq, (1) should also be used.\nLet‚Äôs use both of these and modify our iterate:\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    if log_dir is None or str(log_dir).strip() == \"\": \n        cfg_log_dir = Path(\"log_artifacts\")\n    else:\n        cfg_log_dir = Path(log_dir)\n\n    if model_dir is None or str(model_dir).strip() == \"\": \n        cfg_model_dir = Path(\"model_dir\")\n    else:\n        cfg_model_dir = Path(model_dir)\n    \n    if not cfg_log_dir.exists(): cfg_log_dir.mkdir()\n    if not cfg_model_dir.exists(): cfg_model_dir.mkdir()\n\n    artifact_file = cfg_log_dir / \"log_file.txt\"\n\n    with mlflow.start_run() as run:\n        # Fetch run information\n        run_id = run.info.run_id\n        run = MlflowClient().get_run(run_id=run_id)\n        logged_params = run.data.params\n\n        # Set param dict\n        params_dict = {\n            \"index\": index,\n            \"log_dir\": str(log_dir),\n            \"model_dir\": str(model_dir),\n            \"cfg_log_dir\": str(cfg_log_dir),\n            \"cfg_model_dir\": str(cfg_model_dir)\n        }\n\n        # Log if not already logged\n        for k, v in params_dict.items():\n            if k not in logged_params:\n                mlflow.log_param(key=k, value=str(v))\n\n        # iterate and log\n        iter_and_log(index, artifact_file)\n\n        # binarize and save model\n        with open(cfg_model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\nWith this, we can successfully log parameters using both\n\nmlflow run \npython iterate.py \n\nNotice that there is a difference in the value for model_dir between mlflow run and python iterate.py, but the correct value can be reliably checked from cfg_model_dir.\n\n\n\nLogging Artifacts the Right Way\nWith metrics and parameters logged, let‚Äôs turn our attention to artifact logging. Mlflow provides the methods mlflow.log_artifact and mlflow.log_artifacts to log files and directories respectively. In our code, we create two artifacts: log_file.txt and the binarized function model_pickle (which is also our proxy for a model artifact that we‚Äôd like to save and version).\nIf the intended purpose was to log and preserve artifacts of a run, then an instinctive way is to just log everything as an artifact. We can modify iterate.py to do that\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    if log_dir is None or str(log_dir).strip() == \"\": \n        cfg_log_dir = Path(\"log_artifacts\")\n    else:\n        cfg_log_dir = Path(log_dir)\n\n    if model_dir is None or str(model_dir).strip() == \"\": \n        cfg_model_dir = Path(\"model_dir\")\n    else:\n        cfg_model_dir = Path(model_dir)\n    \n    if not cfg_log_dir.exists(): cfg_log_dir.mkdir()\n    if not cfg_model_dir.exists(): cfg_model_dir.mkdir()\n\n    artifact_file = cfg_log_dir / \"log_file.txt\"\n\n    with mlflow.start_run() as run:\n        # Fetch run information\n        run_id = run.info.run_id\n        run = MlflowClient().get_run(run_id=run_id)\n        logged_params = run.data.params\n\n        # Set param dict\n        params_dict = {\n            \"index\": index,\n            \"log_dir\": str(log_dir),\n            \"model_dir\": str(model_dir),\n            \"cfg_log_dir\": str(cfg_log_dir),\n            \"cfg_model_dir\": str(cfg_model_dir)\n        }\n\n        # Log if not already logged\n        for k, v in params_dict.items():\n            if k not in logged_params:\n                mlflow.log_param(key=k, value=str(v))\n\n        # iterate and log\n        iter_and_log(index, artifact_file)\n\n        # log cfg_log_dir as artifact\n        mlflow.log_artifacts(str(cfg_log_dir), \"log_dir\")\n\n        # binarize and save model\n        with open(cfg_model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\n\n        # log cfg_model_dir as artifact\n        mlflow.log_artifacts(str(cfg_model_dir), \"model_dir\")\n\n\n\nMlflow UI showing logged artifacts\n\n\nBut if the intention is to version the model, the model artifact has to be logged as an mlflow model. To do that we would have to log models using a supported method which can get tricky - it has to be one of the supported methods, and it assumes access to the model itself. Sometimes, especially when working with open-source deep learning libraries (eg. Fairseq, Yolov5) the native logging methods don‚Äôt expose the models themselves but provide means to fetch the location of the saved models.\nIn such cases, a solution might be to load the model again just to log it to mlflow in a form that can be registered. Depending on the model, this could punish the compute and memory.\nAn alternative, that was recently implemented in huggingface by Swetha Mandava, is to use mlflow.pyfunc to log any model with a pyfunc interface.\nWe can use this to change iterate\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    if log_dir is None or str(log_dir).strip() == \"\": \n        cfg_log_dir = Path(\"log_artifacts\")\n    else:\n        cfg_log_dir = Path(log_dir)\n\n    if model_dir is None or str(model_dir).strip() == \"\": \n        cfg_model_dir = Path(\"model_dir\")\n    else:\n        cfg_model_dir = Path(model_dir)\n    \n    if not cfg_log_dir.exists(): cfg_log_dir.mkdir()\n    if not cfg_model_dir.exists(): cfg_model_dir.mkdir()\n\n    artifact_file = cfg_log_dir / \"log_file.txt\"\n\n    with mlflow.start_run() as run:\n        # Fetch run information\n        run_id = run.info.run_id\n        run = MlflowClient().get_run(run_id=run_id)\n        logged_params = run.data.params\n\n        # Set param dict\n        params_dict = {\n            \"index\": index,\n            \"log_dir\": str(log_dir),\n            \"model_dir\": str(model_dir),\n            \"cfg_log_dir\": str(cfg_log_dir),\n            \"cfg_model_dir\": str(cfg_model_dir)\n        }\n\n        # Log if not already logged\n        for k, v in params_dict.items():\n            if k not in logged_params:\n                mlflow.log_param(key=k, value=str(v))\n\n        # iterate and log\n        iter_and_log(index, artifact_file)\n\n        # log cfg_log_dir as artifact\n        mlflow.log_artifacts(str(cfg_log_dir), \"log_dir\")\n\n        # binarize and save model\n        with open(cfg_model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\n\n        # log as mlflow model that can be registered\n        mlflow.pyfunc.log_model(\n            artifact_path=\"iterate_model\", \n            artifacts={\n                \"model_dir\": str(cfg_model_dir)\n            },\n            python_model=mlflow.pyfunc.PythonModel()\n        )\nNow, when this script is run, log.txt is logged as an artifact and the binarized function is logged as a model that can be registered. \n\n\nFinal Thoughts and Conclusion\nThese were some of the tricky parts that I encountered while implementing mlflow logging in few codebases. Another quick tip is to use artifact folders with epochs to log multiple artifacts or models that can be individually registered, but I‚Äôll save elaboration on that for a future post.\nA question I found myself asking very often was ‚Äúdo I need to support both fluent API as well as MLproject‚Äù? The following influences my thoughts on it.\nHamel Husain, in his talk about evaluating ML Tooling, mentions that good tooling should have as little Domain Specific Language (DSL) as possible. And the MLproject file, while it has it‚Äôs uses, is definitely DSL. It is a file that is used only for the purpose of interacting with mlflow cli and it forces users to learn how to specify the file in addition to already learning to use the mlflow python package.\nThat being said, my instinct as a maintainer is to enable usage while also respecting my time. Therefore, while it could be slightly more tedious to enable support for both patterns of running mlflow logging, it could also be worth the extra effort if the codebase could benefit users interacting with it using mlflow.\n\nThe best reason to support fluent API is because it is more natural and requires the user to learn less.\n\nFew great reasons to support MLproject and mlflow cli based logging are:\n\nIf the user is a databricks user, the CLI enables use of infrastructure.\n\nMLproject file can be used to orchestrate workflows."
  }
]