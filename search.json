[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Ganesh and I am a Machine Learning Engineer with expertise in creating tools to ease developing and serving new models."
  },
  {
    "objectID": "posts/mlflow-logging/2022-09-12-mlflow-logging-design-decisions-tricky-parts.html",
    "href": "posts/mlflow-logging/2022-09-12-mlflow-logging-design-decisions-tricky-parts.html",
    "title": "Mlflow : Design Decisions and Tricky Parts",
    "section": "",
    "text": "I have been using Mlflow for a while now to track experimentation and model artifact lineage. While setting up codebases to use mlflow, I had to make some design decisions that influenced the successful logging of all metrics, parameters, artifacts, and the addition of the model artifacts to the model registry.\nBroadly, these were:\n\nThe intended pattern to run projects\n\nLogging artifacts the right way\n\nI wanted to draw out a few of these by adding mlflow logging to a small script. Some of these choices become gotchas while working with large codebases that have long data loading and training periods. Depending on the task and the compute being used, these gotchas could be expensive.\n\nPre-requisites\nLet’s start with a function that doesn’t do any machine learning, it simply takes in a parameter (an integer index) and iterates upto that number while printing the numbers to a file.\ndef iter_and_log(index: int, log_file: Path):\n    with open(log_file, \"w\") as lf:\n        for i in range(index):\n            lf.write(f\"{i}\\n\")\nA commonly followed pattern in machine learning / deep learning training code is:\n\nAccept parameters from cli using packages like argparse or click.\nCreate some artifacts (our log file here is an artifact that we can use)\nAfter performing training (also during), save the model.\n\nFor the purpose of our example, let’s mimic this pattern by doing the following:\n## iterate.py \n\nimport click\nfrom pathlib import Path\nimport pickle\n\n@click.command()\n@click.option(\n    \"--index\",\n    \"-i\",\n    type=click.INT,\n    help=\"Upper limit for iteration\",\n)\n@click.option(\n    \"--log-dir\",\n    \"-L\",\n    type=click.Path(),\n    default=Path(\"log_artifacts\"),\n    help=\"Log artifact file path\"\n)\n@click.option(\n    \"--model-dir\",\n    \"-M\",\n    type=click.Path(),\n    default=Path(\"model_dir\"),\n    help=\"'Model' directory\"\n)\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    # ensure log_dir and model_dir are Path objects\n    log_dir = Path(log_dir)\n    model_dir = Path(model_dir)\n\n    ## Create dirs if they don't exist\n    if not log_dir.exists(): log_dir.mkdir()\n    if not model_dir.exists(): model_dir.mkdir()\n\n    # set log_file path\n    artifact_file = log_dir / \"log_file.txt\"\n\n    # perform iteration and logging\n    iter_and_log(index, artifact_file)\n\n    # serialize and save the function that does serialization and logging\n    # this is our proxy for a model\n    with open(model_dir / \"model_pickle\", \"wb\") as model_file:\n        pickle.dump(iter_and_log, model_file)\n\ndef iter_and_log(index: int, log_file: Path) -&gt; None:\n    \"\"\"Function that does the iteration and logging\n    Iterates to `index` and logs the values to `log_file`\n    \"\"\"\n    with open(log_file, \"w\") as lf:\n        for i in range(index):\n            lf.write(f\"{i}\\n\")\n    \n\nif __name__ == \"__main__\":\n    iterate()\nThis can be run for 10 iterations as follows\npython iterate.py -i 10\nand it will create the artifact directories, iterate and log, and then serialize and dump the function to file.\nNow, with the pre-requisits done, let’s talk about adding mlflow logging and running the project.\n\n\nThe Intended Pattern to Run Projects\nA project that has mlflow logging can be run using two patterns\n\nUsing the python command\n\nIn this pattern, the project is run as it would be normally i.e. its python command or bash script. In our example that would be python iterate.py -i 10.\n\nWhen logging is done this way, mlflow runs in Fluent API mode.\n\nMlflow Projects\n\nIn this pattern, an MLproject file gets defined at the project’s root directory.\n\nTo run the project, the mlflow run command is used to run an entry point and the command to be used is defined within the entry point.\n\n\nAdding logging to the codebase such that both of these patterns can be supported is often tricky! Let’s add logging to our script and see why. The first step is adding the MLproject file:\n## MLproject\n\nname: sample-mlflow-iterate\n\nentry_points:\n  main:\n    parameters:\n      index: int\n      log_dir:\n        default: log_artifacts\n      model_dir:\n        default: model_dir\n\n    command: python iterate.py -i {index} --log-dir {log_dir} --model-dir {model_dir}\nOnce, the file has been added to the project root, we can use mlflow cli to run the project as\nmlflow run . -P index=10 --env-manager=local\nwhere -P is how a parameter is specified while running and --env-manager=local is to tell mlflow to use the local environment.\nCircling back, our project that will use mlflow logging, is able to run in two ways:\n1. Using the python command python iterate.py -i 10, in which case mlflow will run as Fluent API.\n2. Using the Mlproject file and mlflow cli as mlflow run . -P index=10 --env-manager=local.\nNow, let’s add logging and see the gotchas depending on how we run the projects.\n\nStarting a run and logging metrics\nTo start logging to a run, an mlflow run has to be started in the code. We can do that using mlflow.start_run() either as a context using the with keyword or by itself. The documentation for this function does it using the with keyword - which also takes care of ending the run. But in many codebases (eg. huggingface transformers) encapsulating the entire logging process inside of one context is not possible, and therefore mlflow.start_run() is used by itself quite often. When we do that, we also need to take care that we end the run using mlflow.end_run().\nOur script is small, so let’s use the with keyword:\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    # ensure log_dir and model_dir are Path objects\n    log_dir = Path(log_dir)\n    model_dir = Path(model_dir)\n\n    ## Create dirs if they don't exist\n    if not log_dir.exists(): log_dir.mkdir()\n    if not model_dir.exists(): model_dir.mkdir()\n\n    # set log_file path\n    artifact_file = log_dir / \"log_file.txt\"\n\n    # start mlflow run\n    with mlflow.start_run():\n\n        # perform iteration and logging\n        iter_and_log(index, artifact_file)\n\n        # serialize and save the function that does serialization and logging\n        # this is our proxy for a model\n        with open(model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\nand to log some metrics, we can use mlflow.log_metric() function within the iter_and_log function as follows:\ndef iter_and_log(index: int, log_file: Path) -&gt; None:\n    \"\"\"Function that does the iteration and logging\n    Iterates to `index` and logs the values to `log_file`\n    \"\"\"\n    with open(log_file, \"w\") as lf:\n        for i in range(index):\n            # log to mlflow\n            mlflow.log_metric(\n                key=\"current_iter\",\n                value=i,\n                step=i # Don't forget to add step, absence makes all logs to step 0\n            )\n\n            lf.write(f\"{i}\\n\")\nNow when we run this project using mlflow run, we can see that mlflow logs the input parameters (specified in MLproject) and the metrics as we intended. \nBut when we run the same using the python command, we see that the parameters are not logged. \nFixing that should be as easy as using mlflow.log_param() in the code, right? In this script - yes. Let’s add that before we talk about where that could be insufficient. We can modify the iterate function to add parameter logging.\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    # ensure log_dir and model_dir are Path objects\n    log_dir = Path(log_dir)\n    model_dir = Path(model_dir)\n\n    ## Create dirs if they don't exist\n    if not log_dir.exists(): log_dir.mkdir()\n    if not model_dir.exists(): model_dir.mkdir()\n\n    # set log_file path\n    artifact_file = log_dir / \"log_file.txt\"\n\n    # start mlflow run\n    with mlflow.start_run():\n        # Log parameters\n        mlflow.log_param(key=\"index\", value=index)\n        mlflow.log_param(key=\"log_dir\", value=str(log_dir))\n        mlflow.log_param(key=\"model_dir\", value=str(model_dir))\n\n        # perform iteration and logging\n        iter_and_log(index, artifact_file)\n\n        # serialize and save the function that does serialization and logging\n        # this is our proxy for a model\n        with open(model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\nand with this we can run the script using the python command and even the parameters will be logged. \n\n\nReasonable defaults and logging parameters\nIn Mlflow, the intended use for logging parameters seems to be only for the purpose of capturing initial parameters and hyperparameters. Unlike metric logging, the parameter logging functions don’t have a step parameter to capture parameter values as they change. There is also no way to update a subset of parameters which is possible in some other logging libraries such as Weights and Biases. In mlflow, (hyper)parameters that change over time should be logged as metrics (if they are numeric) or to a nested run as recommended by the package.\nBut a common practice in many codebases is to set reasonable defaults and rely on those to be used when an empty string or a null value is passed as an initial parameter. Codebases such as fairseq have dozens of parameters and it’s not realistic to assume that a user would set them all when running the code. Likewise, it’s not realistic to assume that a user would write them all in the MLproject file. A more realistic assumption is that the user would specify some of the parameters and rely on the default values for the remaining.\nLet’s mimic this pattern in our script and see what happens.\nFirst we modify our iterate function as\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    ## set reasonable defaults\n    if log_dir is None or str(log_dir).strip() == \"\": log_dir = Path(\"log_artifacts\")\n    if model_dir is None or str(model_dir).strip() == \"\": model_dir = Path(\"model_dir\")\n\n    # ensure log_dir and model_dir are Path objects\n    log_dir = Path(log_dir)\n    model_dir = Path(model_dir)\n\n    ## Create dirs if they don't exist\n    if not log_dir.exists(): log_dir.mkdir()\n    if not model_dir.exists(): model_dir.mkdir()\n\n    # set log_file path\n    artifact_file = log_dir / \"log_file.txt\"\n\n    # start mlflow run\n    with mlflow.start_run():\n\n        # perform iteration and logging\n        iter_and_log(index, artifact_file)\n\n        # serialize and save the function that does serialization and logging\n        # this is our proxy for a model\n        with open(model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\nWe know that this already works using the python command because we have been using the defaults specified using click. But in the MLproject file, we have specified all the parameters with their defaults. Let’s modify it to get the code to rely on default values.\n## MLproject\n\nname: sample-mlflow-iterate\n\nentry_points:\n  main:\n    parameters:\n      index: int\n      model_dir:\n        default: null\n\n    command: python iterate.py -i {index} --model-dir {model_dir}\nI modified the MLproject file to remove log_dir altogether and set model_dir to null. When we now run this code using mlflow run . -P index=10 --env-manager=local, turns out it is not allowed:\n2022/09/11 18:33:40 ERROR mlflow.cli: === No value given for missing parameters: 'model_dir' ===\nWe cannot set parameters to null. This means, if a codebase requires some parameter be set to None to get the code to use a default values, leave it out of the MLproject file. This might actually be impossible in many codebases and should be taken under consideration when deciding to support running the project using the MLproject file.\nLuckily, our script also replaces an empty string with defaults. Therefore, we should be able to modify the MLproject file and set model_dir to ''.\n## MLproject\n\nname: sample-mlflow-iterate\n\nentry_points:\n  main:\n    parameters:\n      index: int\n      model_dir:\n        default: \"\"\n\n    command: python iterate.py -i {index} --model-dir {model_dir}\nWhen we now run this, we still run into an error:\nmlflow.exceptions.MlflowException: Changing param values is not allowed. \\ \nParam with key='model_dir' was already logged with \\\nvalue='''' for run ID='7e8922428cf5464582b6f44fb3a183ab'. \\ \nAttempted logging new value 'model_dir'.\nThis is because model_dir was already logged from MLProject file and so when we log the value we finally use, it raises the error to indicate that updating a parameter is not allowed.\nIn my opinion, there is no ideal way to fix this. I dislike the idea of logging this value to a nested run, since there is no need. The parameter did not evolve over time to trigger a search in a new hyperparameter space, this is just getting the code to use a default value. But here are a few possible ways to address this:\n1. Ignore logging the parameters that have already been logged\nHere is some code that could do that assuming the parameters have been specified as a dictionary in the code:\nfrom mlflow.tracking import MlflowClient\n\nrun_id = mlflow.active_run().info.run_id # get run_id of active run\nrun = self.client.get_run(run_id=self.run_id) # get current run data\nlogged_params = run.data.params # get already logged params as a dict\nfor k, v in params_dict.items():\n    if k not in logged_params:\n        mlflow.log_param(key=k, value=str(v))\n2. Use different names for parameters and the command line arguments\nThis allows both, command line arguments that are initial parameters, as well as the values that actually get used to be logged. Example: Fairseq uses the command line arguments to set the configs that are actually used in code. It also provides the cli args as part of the cfg variable, and so when logging parameters using mlflow for fairseq, (1) should also be used.\nLet’s use both of these and modify our iterate:\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    if log_dir is None or str(log_dir).strip() == \"\": \n        cfg_log_dir = Path(\"log_artifacts\")\n    else:\n        cfg_log_dir = Path(log_dir)\n\n    if model_dir is None or str(model_dir).strip() == \"\": \n        cfg_model_dir = Path(\"model_dir\")\n    else:\n        cfg_model_dir = Path(model_dir)\n    \n    if not cfg_log_dir.exists(): cfg_log_dir.mkdir()\n    if not cfg_model_dir.exists(): cfg_model_dir.mkdir()\n\n    artifact_file = cfg_log_dir / \"log_file.txt\"\n\n    with mlflow.start_run() as run:\n        # Fetch run information\n        run_id = run.info.run_id\n        run = MlflowClient().get_run(run_id=run_id)\n        logged_params = run.data.params\n\n        # Set param dict\n        params_dict = {\n            \"index\": index,\n            \"log_dir\": str(log_dir),\n            \"model_dir\": str(model_dir),\n            \"cfg_log_dir\": str(cfg_log_dir),\n            \"cfg_model_dir\": str(cfg_model_dir)\n        }\n\n        # Log if not already logged\n        for k, v in params_dict.items():\n            if k not in logged_params:\n                mlflow.log_param(key=k, value=str(v))\n\n        # iterate and log\n        iter_and_log(index, artifact_file)\n\n        # binarize and save model\n        with open(cfg_model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\nWith this, we can successfully log parameters using both\n\nmlflow run \npython iterate.py \n\nNotice that there is a difference in the value for model_dir between mlflow run and python iterate.py, but the correct value can be reliably checked from cfg_model_dir.\n\n\n\nLogging Artifacts the Right Way\nWith metrics and parameters logged, let’s turn our attention to artifact logging. Mlflow provides the methods mlflow.log_artifact and mlflow.log_artifacts to log files and directories respectively. In our code, we create two artifacts: log_file.txt and the binarized function model_pickle (which is also our proxy for a model artifact that we’d like to save and version).\nIf the intended purpose was to log and preserve artifacts of a run, then an instinctive way is to just log everything as an artifact. We can modify iterate.py to do that\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    if log_dir is None or str(log_dir).strip() == \"\": \n        cfg_log_dir = Path(\"log_artifacts\")\n    else:\n        cfg_log_dir = Path(log_dir)\n\n    if model_dir is None or str(model_dir).strip() == \"\": \n        cfg_model_dir = Path(\"model_dir\")\n    else:\n        cfg_model_dir = Path(model_dir)\n    \n    if not cfg_log_dir.exists(): cfg_log_dir.mkdir()\n    if not cfg_model_dir.exists(): cfg_model_dir.mkdir()\n\n    artifact_file = cfg_log_dir / \"log_file.txt\"\n\n    with mlflow.start_run() as run:\n        # Fetch run information\n        run_id = run.info.run_id\n        run = MlflowClient().get_run(run_id=run_id)\n        logged_params = run.data.params\n\n        # Set param dict\n        params_dict = {\n            \"index\": index,\n            \"log_dir\": str(log_dir),\n            \"model_dir\": str(model_dir),\n            \"cfg_log_dir\": str(cfg_log_dir),\n            \"cfg_model_dir\": str(cfg_model_dir)\n        }\n\n        # Log if not already logged\n        for k, v in params_dict.items():\n            if k not in logged_params:\n                mlflow.log_param(key=k, value=str(v))\n\n        # iterate and log\n        iter_and_log(index, artifact_file)\n\n        # log cfg_log_dir as artifact\n        mlflow.log_artifacts(str(cfg_log_dir), \"log_dir\")\n\n        # binarize and save model\n        with open(cfg_model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\n\n        # log cfg_model_dir as artifact\n        mlflow.log_artifacts(str(cfg_model_dir), \"model_dir\")\n\n\n\nMlflow UI showing logged artifacts\n\n\nBut if the intention is to version the model, the model artifact has to be logged as an mlflow model. To do that we would have to log models using a supported method which can get tricky - it has to be one of the supported methods, and it assumes access to the model itself. Sometimes, especially when working with open-source deep learning libraries (eg. Fairseq, Yolov5) the native logging methods don’t expose the models themselves but provide means to fetch the location of the saved models.\nIn such cases, a solution might be to load the model again just to log it to mlflow in a form that can be registered. Depending on the model, this could punish the compute and memory.\nAn alternative, that was recently implemented in huggingface by Swetha Mandava, is to use mlflow.pyfunc to log any model with a pyfunc interface.\nWe can use this to change iterate\ndef iterate(index: int, log_dir: Path, model_dir: Path) -&gt; None:\n    if log_dir is None or str(log_dir).strip() == \"\": \n        cfg_log_dir = Path(\"log_artifacts\")\n    else:\n        cfg_log_dir = Path(log_dir)\n\n    if model_dir is None or str(model_dir).strip() == \"\": \n        cfg_model_dir = Path(\"model_dir\")\n    else:\n        cfg_model_dir = Path(model_dir)\n    \n    if not cfg_log_dir.exists(): cfg_log_dir.mkdir()\n    if not cfg_model_dir.exists(): cfg_model_dir.mkdir()\n\n    artifact_file = cfg_log_dir / \"log_file.txt\"\n\n    with mlflow.start_run() as run:\n        # Fetch run information\n        run_id = run.info.run_id\n        run = MlflowClient().get_run(run_id=run_id)\n        logged_params = run.data.params\n\n        # Set param dict\n        params_dict = {\n            \"index\": index,\n            \"log_dir\": str(log_dir),\n            \"model_dir\": str(model_dir),\n            \"cfg_log_dir\": str(cfg_log_dir),\n            \"cfg_model_dir\": str(cfg_model_dir)\n        }\n\n        # Log if not already logged\n        for k, v in params_dict.items():\n            if k not in logged_params:\n                mlflow.log_param(key=k, value=str(v))\n\n        # iterate and log\n        iter_and_log(index, artifact_file)\n\n        # log cfg_log_dir as artifact\n        mlflow.log_artifacts(str(cfg_log_dir), \"log_dir\")\n\n        # binarize and save model\n        with open(cfg_model_dir / \"model_pickle\", \"wb\") as model_file:\n            pickle.dump(iter_and_log, model_file)\n\n        # log as mlflow model that can be registered\n        mlflow.pyfunc.log_model(\n            artifact_path=\"iterate_model\", \n            artifacts={\n                \"model_dir\": str(cfg_model_dir)\n            },\n            python_model=mlflow.pyfunc.PythonModel()\n        )\nNow, when this script is run, log.txt is logged as an artifact and the binarized function is logged as a model that can be registered. \n\n\nFinal Thoughts and Conclusion\nThese were some of the tricky parts that I encountered while implementing mlflow logging in few codebases. Another quick tip is to use artifact folders with epochs to log multiple artifacts or models that can be individually registered, but I’ll save elaboration on that for a future post.\nA question I found myself asking very often was “do I need to support both fluent API as well as MLproject”? The following influences my thoughts on it.\nHamel Husain, in his talk about evaluating ML Tooling, mentions that good tooling should have as little Domain Specific Language (DSL) as possible. And the MLproject file, while it has it’s uses, is definitely DSL. It is a file that is used only for the purpose of interacting with mlflow cli and it forces users to learn how to specify the file in addition to already learning to use the mlflow python package.\nThat being said, my instinct as a maintainer is to enable usage while also respecting my time. Therefore, while it could be slightly more tedious to enable support for both patterns of running mlflow logging, it could also be worth the extra effort if the codebase could benefit users interacting with it using mlflow.\n\nThe best reason to support fluent API is because it is more natural and requires the user to learn less.\n\nFew great reasons to support MLproject and mlflow cli based logging are:\n\nIf the user is a databricks user, the CLI enables use of infrastructure.\n\nMLproject file can be used to orchestrate workflows."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ganeshscribbles",
    "section": "",
    "text": "Mlflow : Design Decisions and Tricky Parts\n\n\n\n\n\n\nexperimentation\n\n\nmlflow\n\n\npython\n\n\n\nChoices to keep in mind while adding mlflow logging to a codebase\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\nNo matching items"
  }
]